{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuchLearningSuchWow - LSTM - Crossvalidation\n",
    "\n",
    "This notebook contains the code we used to run crossvalidation to find the best parameters for our network. The training code is based loosely on [this kernel](https://www.kaggle.com/bountyhunters/baseline-lstm-with-keras-0-7). The WRMSSE callback uses the same WRMSSE evaluator as the one used in `LSTM_Evaluation.ipynb`, which is based on [this discussion](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834). The Time Series Split used in this kernel was obtained from [this discussion](https://www.kaggle.com/mpearmain/extended-timeseriessplitter) from another Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from WRMSSEEvaluator import WRMSSEEvaluator\n",
    "from TimeSeriesSplit import TimeSeriesSplit\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"input/m5-forecasting-accuracy/\"\n",
    "outputPath = \"output/\"\n",
    "modelPath = \"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 14 # Number of previous days that will be used to predict the next day\n",
    "startDay = 1000 # Number of days at start of data that will be ignored during training\n",
    "testSize = 28 # Number of days that will be used as the test set for every fold\n",
    "useTestWhenFittingScaler = True # Whether or not to use the test fold when fitting the scaler \n",
    "                                # (fixes large spread in validation loss)\n",
    "\n",
    "# Model settings\n",
    "add_1dConv = True\n",
    "lstm_units = (50, 400, 400)\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Rolling means\n",
    "add_rollingMeans = False\n",
    "\n",
    "# Crossvalidation loop settings\n",
    "nr_folds = 5\n",
    "nr_epochs_per_fold = 40\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputPath + \"/preprocessed_train_valid_data.pkl\", \"rb\") as f:\n",
    "    df_train_valid = pickle.load(f)\n",
    "with open(outputPath + \"/item_data.pkl\", \"rb\") as f:\n",
    "    item_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_valid[:-28] # Remove Kaggle validation data\n",
    "del df_train_valid\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM():\n",
    "    # Note: this function is identical to the one in LSTM_Training.ipynb\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # 1D convolution layer\n",
    "    if(add_1dConv):\n",
    "        model.add(Conv1D(filters=32, kernel_size=7, strides=1, \n",
    "                         padding=\"causal\", activation=\"relu\",\n",
    "                         input_shape=(item_data.shape[0]+timesteps, # == X_train.shape[1]\n",
    "                                      df_train.shape[1]))) # == X_train.shape[2]  \n",
    "\n",
    "    # LSTM layers\n",
    "    layer_1_units, layer_2_units, layer_3_units = lstm_units\n",
    "    \n",
    "    if(add_1dConv):\n",
    "        model.add(LSTM(units = layer_1_units, return_sequences = True))\n",
    "    else:\n",
    "        model.add(LSTM(units = layer_1_units, \n",
    "                       return_sequences = True, \n",
    "                       input_shape=(item_data.shape[0]+timesteps, df_train.shape[1])))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(LSTM(units = layer_2_units, return_sequences = True))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(LSTM(units = layer_3_units))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(units = 30490))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_LSTM()\n",
    "plot_model(model, modelPath + \"/model.png\")\n",
    "print(model.summary())\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRMSSE Evaluation Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSECallback(Callback):\n",
    "    \n",
    "    def __init__( self, df_train_orginal, df_valid_original, df_calendar, df_prices, timesteps, \n",
    "                  input_data, item_data, scaler, nr_days_to_predict, additional_features, val_start_day):\n",
    "        # Create evaluator and set all values necessary for testing & evaluation\n",
    "        self.evaluator = WRMSSEEvaluator(df_train_orginal, df_valid_original, df_calendar, df_prices)\n",
    "        self.timesteps = timesteps\n",
    "        self.input_data = input_data\n",
    "        self.item_data = item_data\n",
    "        self.scaler = scaler\n",
    "        self.nr_days_to_predict = nr_days_to_predict\n",
    "        self.additional_features = additional_features\n",
    "        self.val_start_day = val_start_day\n",
    "        \n",
    "    def compute_rolling_means(self, test_data, predictions):\n",
    "        # Note: this function is identical to the one in LSTM_Testing.ipynb\n",
    "        \n",
    "        # Compute the next row of rolling means (this implementation works because input_data contains more than 7 days)\n",
    "        test_data = np.squeeze(test_data)\n",
    "        df_test_pred = pd.DataFrame(np.concatenate([test_data[:,:30490], predictions], axis = 0))\n",
    "        rolling_mean = pd.DataFrame(df_test_pred.rolling(7).mean())\n",
    "        rolling_mean = rolling_mean.fillna(0)\n",
    "\n",
    "        return rolling_mean[-1:]\n",
    "        \n",
    "    def test_model( self, model ):\n",
    "        # Note: this function is identical to the one in LSTM_Testing.ipynb\n",
    "        \n",
    "        # Scale and convert input data so that it can be fed into the model\n",
    "        inputs = self.scaler.transform(self.input_data)\n",
    "        X_test = np.array([inputs])\n",
    "    \n",
    "        # Predict sales for the next nr_days_to_predict days\n",
    "        predictions = []\n",
    "        for j in range(0, self.nr_days_to_predict):\n",
    "            feature_shape = 30490 + additional_features.shape[1]\n",
    "            if(add_rollingMeans):\n",
    "                feature_shape += 30490 # If rolling means are present, feature_shape is 30490 + # additional features + 30490\n",
    "            model_input = np.append(np.expand_dims(item_data, 0), \n",
    "                                    X_test[:,-self.timesteps:,:].reshape(1, self.timesteps, feature_shape), axis = 1)\n",
    "            predicted_sales = model.predict(model_input)\n",
    "            to_stack = [np.array(predicted_sales), self.additional_features.iloc[[j]]]\n",
    "            if(add_rollingMeans): # If rolling means are required, compute them and add them to model_output\n",
    "                rolling_means = self.compute_rolling_means(X_test, predicted_sales)\n",
    "                to_stack.append(rolling_means)\n",
    "            model_output = np.column_stack(tuple(to_stack))\n",
    "            model_output_expanded = np.expand_dims(model_output, 0)\n",
    "            X_test = np.append(X_test, model_output_expanded, axis = 1)\n",
    "            predicted_sales = self.scaler.inverse_transform(model_output)[:,0:30490]\n",
    "            predictions.append(predicted_sales)\n",
    "    \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate( self, predictions ):\n",
    "        # Reshape predictions in order to feed them to the evaluator\n",
    "        predictions = pd.DataFrame(data=np.array(predictions).reshape(self.nr_days_to_predict,30490))\n",
    "        predictions = predictions.T\n",
    "        predictions.columns = [f\"d_{i}\" for i in range(self.val_start_day,self.val_start_day + self.nr_days_to_predict)]\n",
    "        \n",
    "        # Compute and return WRMSSE\n",
    "        groups, scores = self.evaluator.score(predictions)\n",
    "        wrmsse_score = np.mean(scores)\n",
    "        \n",
    "        return wrmsse_score\n",
    "    \n",
    "    def on_epoch_end( self, epoch, logs=None ):\n",
    "        logs = logs or {}\n",
    "        # Predict using the current state of the model, compute WRMSSE, add the result to the log and print it\n",
    "        predictions = self.test_model(self.model)\n",
    "        wrmsse_score = self.evaluate(predictions)\n",
    "        logs['val_wrmsse'] = wrmsse_score\n",
    "        print(\"Validation WRMSSE after epoch \"+str(epoch+1)+\": \"+str(wrmsse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRMSSE evaluator requires original, non-preprocessed data\n",
    "df_sales = pd.read_csv(inputPath + \"/sales_train_evaluation.csv\")\n",
    "df_calendar = pd.read_csv(inputPath + \"/calendar.csv\")\n",
    "df_prices = pd.read_csv(inputPath + \"/sell_prices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (RMSE)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series splitter\n",
    "tss = TimeSeriesSplit(nr_folds, test_size=testSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cross Validation Loop\n",
    "histories = []\n",
    "for fold, (train_index, test_index) in enumerate(tss.split(range(0,df_train.shape[0]-timesteps))):\n",
    "    # Change fold from index to number and print the current fold\n",
    "    fold += 1\n",
    "    print(\"Fold \"+str(fold)+\"/\"+str(nr_folds))\n",
    "\n",
    "    # Create scaled data for the current fold (test data is used in fit as well, to make rmse more meaningful)\n",
    "    print(\"Scaling data...\")\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    if(useTestWhenFittingScaler):\n",
    "        df_split_scaled = scaler.fit_transform(df_train[:timesteps+len(train_index)+len(test_index)])\n",
    "    else:\n",
    "        df_train_split_scaled = scaler.fit_transform(df_train[:timesteps+len(train_index)])\n",
    "        df_test_split_scaled = scaler.transform(df_train[timesteps+len(train_index):timesteps+len(train_index)+len(test_index)])\n",
    "        df_split_scaled = np.append(df_train_split_scaled, df_test_split_scaled, axis = 0)\n",
    "    \n",
    "    # Create training and testing sets for the current split\n",
    "    print(\"Creating training and testing splits...\")\n",
    "    X_train_split, y_train_split, X_test_split, y_test_split = [],[],[],[]\n",
    "    for i in range(timesteps, timesteps+len(train_index)):\n",
    "        X_train_split.append(np.append(item_data, df_split_scaled[i-timesteps:i], axis = 0))\n",
    "        y_train_split.append(df_split_scaled[i][0:30490]) # Only use first 30490 columns (sales) as labels\n",
    "    for i in range(timesteps+len(train_index), timesteps+len(train_index)+len(test_index)):\n",
    "        X_test_split.append(np.append(item_data, df_split_scaled[i-timesteps:i], axis = 0))\n",
    "        y_test_split.append(df_split_scaled[i][0:30490]) # Only use first 30490 columns (sales) as labels\n",
    "    del df_split_scaled\n",
    "    \n",
    "    # Convert training and testing sets to numpy arrays so that they can be fed to the model\n",
    "    X_train_split = np.array(X_train_split)\n",
    "    y_train_split = np.array(y_train_split)\n",
    "    X_test_split = np.array(X_test_split)\n",
    "    y_test_split = np.array(y_test_split)\n",
    "    \n",
    "    # Create a new model\n",
    "    model = create_LSTM()\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer = Adam(learning_rate=learning_rate), loss = root_mean_squared_error)\n",
    "    \n",
    "    # Create WRMSSE evaluation callback\n",
    "    val_start_day = startDay+len(train_index)+timesteps+1\n",
    "    df_train_original = df_sales\n",
    "    df_valid_original = df_sales.iloc[:, val_start_day+5:val_start_day+5+testSize]\n",
    "    input_data = df_train[len(train_index):len(train_index)+timesteps]\n",
    "    additional_features = df_train[len(train_index)+timesteps:len(train_index)+timesteps+testSize].iloc[:, 30490:]\n",
    "    if(add_rollingMeans):\n",
    "        additional_features = additional_features.iloc[:, :-30490]\n",
    "    print(\"Creating WRMSSE callback...\")\n",
    "    wrmsse_valid = WRMSSECallback( df_train_original, df_valid_original, df_calendar, df_prices, timesteps, \n",
    "                                   input_data, item_data, scaler, testSize, additional_features, val_start_day)\n",
    "    \n",
    "    # Create callbacks that save the models with the lowest validation loss and WRMSSE score\n",
    "    mcp_save_loss = ModelCheckpoint(modelPath + \"/lstm_model_best_loss_\"+str(fold), \n",
    "                                    save_best_only=True, monitor='val_loss', mode='min')\n",
    "    mcp_save_wrmsse = ModelCheckpoint(modelPath + \"/lstm_model_best_wrmsse_\"+str(fold), \n",
    "                                      save_best_only=True, monitor='val_wrmsse', mode='min')\n",
    "        \n",
    "    # Fit the model\n",
    "    hist = model.fit(X_train_split, y_train_split, \n",
    "                     epochs = nr_epochs_per_fold, \n",
    "                     validation_data = (X_test_split, y_test_split), \n",
    "                     batch_size = batch_size, \n",
    "                     verbose = 1,\n",
    "                     callbacks = [wrmsse_valid, mcp_save_loss, mcp_save_wrmsse])\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save(modelPath + \"/lstm_model_final_\"+str(fold))\n",
    "    \n",
    "    # Store history and perform garbage collection\n",
    "    histories.append(hist)\n",
    "    del model\n",
    "    del X_train_split\n",
    "    del y_train_split\n",
    "    del X_test_split\n",
    "    del y_test_split\n",
    "    del val_start_day\n",
    "    del df_train_original\n",
    "    del df_valid_original\n",
    "    del input_data\n",
    "    del additional_features\n",
    "    del wrmsse_valid\n",
    "    del mcp_save_loss\n",
    "    del mcp_save_wrmsse\n",
    "    del scaler\n",
    "    gc.collect()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_wrmsses = []\n",
    "for fold in histories:\n",
    "    train_losses.append(fold.history['loss'])\n",
    "    val_losses.append(fold.history['val_loss'])\n",
    "    val_wrmsses.append(fold.history['val_wrmsse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lowest average training loss: {:f} after epoch {:d}\"\n",
    "      .format(np.min(np.mean(train_losses, axis = 0)),\n",
    "              np.argmin(np.mean(train_losses, axis = 0))+1))\n",
    "print(\"Lowest average validation loss: {:f} after epoch {:d}\"\n",
    "      .format(np.min(np.mean(val_losses, axis = 0)),\n",
    "              np.argmin(np.mean(val_losses, axis = 0))+1))\n",
    "print(\"Lowest average validation WRMSSE: {:f} after epoch {:d}\"\n",
    "      .format(np.min(np.mean(val_wrmsses, axis = 0)),\n",
    "              np.argmin(np.mean(val_wrmsses, axis = 0))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, nr_epochs, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid()\n",
    "    \n",
    "    ax.plot(np.arange(1, nr_epochs+1), np.mean(train_losses, axis = 0), label='train_loss', color = 'blue')\n",
    "    ax.plot(np.arange(1, nr_epochs+1), np.mean(val_losses, axis = 0), label='val_loss', color = 'red')\n",
    "    \n",
    "    max_train_loss = np.amax(train_losses, axis = 0)\n",
    "    min_train_loss = np.amin(train_losses, axis = 0)\n",
    "    ax.fill_between(np.arange(1, nr_epochs+1), max_train_loss, min_train_loss, alpha = 0.2, color = 'blue')\n",
    "    \n",
    "    max_val_loss = np.max(val_losses, axis = 0)\n",
    "    min_val_loss = np.min(val_losses, axis = 0)\n",
    "    ax.fill_between(np.arange(1, nr_epochs+1), max_val_loss, min_val_loss, alpha = 0.2, color = 'red')\n",
    "    \n",
    "    ax.title.set_text(title)\n",
    "    ax.set_xlabel('Epoch #')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_wrmsse(val_wrmsses, nr_epochs, title):\n",
    "    fix, ax = plt.subplots()\n",
    "    ax.grid()\n",
    "    \n",
    "    ax.plot(np.arange(1, nr_epochs+1), np.mean(val_wrmsses, axis = 0), label='val_wrmsse', color = 'green')\n",
    "    \n",
    "    max_val_wrmsse = np.amax(val_wrmsses, axis = 0)\n",
    "    min_val_wrmsse = np.amin(val_wrmsses, axis = 0)\n",
    "    ax.fill_between(np.arange(1, nr_epochs+1), max_val_wrmsse, min_val_wrmsse, alpha = 0.2, color = 'green')\n",
    "    \n",
    "    ax.title.set_text(title)\n",
    "    ax.set_xlabel('Epoch #')\n",
    "    ax.set_ylabel('WRMSSE')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, val_losses, nr_epochs_per_fold, 'Training & Validation Losses')\n",
    "plot_wrmsse(val_wrmsses, nr_epochs_per_fold, 'Validation WRMSSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputPath + \"/histories.pkl\", \"wb\") as f:\n",
    "    pickle.dump(histories, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
