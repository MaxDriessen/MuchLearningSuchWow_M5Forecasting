{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuchLearningSuchWow - LSTM - Preprocessing\n",
    "\n",
    "This notebook contains the code we used to preprocess the data. The preprocessing code is based primarily on [this kernel](https://www.kaggle.com/bountyhunters/baseline-lstm-with-keras-0-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"input/m5-forecasting-accuracy/\"\n",
    "outputPath = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDay = 1000 # Number of days at start of data that will be ignored during training\n",
    "\n",
    "# Calendar features\n",
    "add_oneDayBeforeEvent = True\n",
    "add_weekend = True\n",
    "add_weekDay = False\n",
    "add_snapDays = False\n",
    "add_months = True\n",
    "\n",
    "# One-hot encoding of item category and state\n",
    "add_categoricalOneHot = False\n",
    "\n",
    "# Rolling means\n",
    "add_rollingMeans = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputPath + \"/downcasted_sales_train_evaluation.pkl\", \"rb\") as f:\n",
    "    df_sales = pickle.load(f)\n",
    "df_calendar = pd.read_csv(inputPath + \"/calendar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the training data so that rows represent days and columns represent items\n",
    "df_sales = df_sales.T\n",
    "print(df_sales.shape)\n",
    "df_sales.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id, item_id, dept_id, cat_id, store_id and state_id rows, as well as the first \"startDay\" days\n",
    "item_data = df_sales[:6] # Save rows for possible future use\n",
    "df_sales = df_sales[6 + startDay:]\n",
    "print(df_sales.shape)\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One day before event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with zeros for 1969 days in the calendar\n",
    "days_before_event = pd.DataFrame(np.zeros((1969,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign \"1\" to days before which there is an event_name_1 (event_name_2 never occurs without event_name_1, so it is redundant)\n",
    "for x,y in df_calendar.iterrows():\n",
    "    if((pd.isnull(df_calendar[\"event_name_1\"][x])) == False and x != 0):\n",
    "        days_before_event[0][x-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with zeros for 1969 days in the calendar\n",
    "weekend = pd.DataFrame(np.zeros((1969,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign \"1\" to saturdays and sundays\n",
    "for x,y in df_calendar.iterrows():\n",
    "    if(df_calendar[\"weekday\"][x] == \"Saturday\" or df_calendar[\"weekday\"][x] == \"Sunday\"):\n",
    "        weekend[0][x] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select week day column from the calendar\n",
    "week_day = df_calendar[[\"wday\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Snap Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select snap day columns from the calendar\n",
    "snap_days = df_calendar[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with zeros for 1969 days in the calendar, with one column for each month\n",
    "months = pd.DataFrame(np.zeros((1969,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign \"1\" in the correct column for each day\n",
    "for x,y in df_calendar.iterrows():\n",
    "    months[df_calendar[\"month\"][x] - 1][x] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Calendar Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_add = []\n",
    "feature_columns = []\n",
    "\n",
    "if(add_oneDayBeforeEvent):\n",
    "    features_to_add.append(days_before_event)\n",
    "    feature_columns.append(\"one_day_before_event\")\n",
    "if(add_weekend):\n",
    "    features_to_add.append(weekend)\n",
    "    feature_columns.append(\"weekend\")\n",
    "if(add_weekDay):\n",
    "    features_to_add.append(week_day)\n",
    "    feature_columns.append(\"week_day\")\n",
    "if(add_snapDays):\n",
    "    features_to_add.append(snap_days)\n",
    "    feature_columns.extend([\"snap_CA\", \"snap_TX\", \"snap_WI\"])\n",
    "if(add_months):\n",
    "    features_to_add.append(months)\n",
    "    feature_columns.extend([\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"])\n",
    "\n",
    "additional_features = pd.DataFrame(np.concatenate(features_to_add, axis = 1))\n",
    "print(additional_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split additional_features into training, validation and evaluation parts\n",
    "additional_features_train_valid = additional_features[startDay:1941]\n",
    "additional_features_valid = additional_features[1913:1941]\n",
    "additional_features_eval  = additional_features[1941:1969]\n",
    "del additional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names of the train + validation part to match training data and match indices\n",
    "additional_features_train_valid.columns = feature_columns\n",
    "additional_features_train_valid.index = df_sales.index\n",
    "additional_features_train_valid.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional_features_train_valid to the sales data to obtain the training & validation set\n",
    "df_train_valid = pd.concat([df_sales, additional_features_train_valid], axis = 1)\n",
    "df_train_valid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(add_rollingMeans):\n",
    "    rolling_mean = pd.DataFrame(df_sales.rolling(7).mean())\n",
    "    rolling_mean = rolling_mean.fillna(0)\n",
    "     \n",
    "    rm_column_names = []\n",
    "    for i in range(30490):\n",
    "        rm_column_names.append(\"rm\"+str(i))\n",
    "    rolling_mean.columns = rm_column_names\n",
    "    \n",
    "    df_train_valid = pd.concat([df_train_valid, rolling_mean], axis = 1)\n",
    "    print(df_train_valid.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Categorical Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(add_categoricalOneHot):\n",
    "    # Create a one-hot encoding of categories and states for each of the 30490 columns, \n",
    "    # with 0s for the additional data columns and the same one-hot encoding for the rolling means\n",
    "    item_data = item_data.iloc[[3,5]]\n",
    "    item_data_one_hot = pd.DataFrame(np.zeros((6,30490)))\n",
    "    unique_categories_states = np.unique(item_data.iloc[0]).tolist() + np.unique(item_data.iloc[1]).tolist()\n",
    "    for i in range(0, item_data.shape[1]):\n",
    "        item_data_one_hot[unique_categories_states.index(item_data[i][0])][i] = 1\n",
    "        item_data_one_hot[unique_categories_states.index(item_data[i][1])][i] = 1\n",
    "    item_data = pd.concat([item_data_one_hot, pd.DataFrame(np.zeros((6, additional_features_train_valid.shape[1])))], axis = 1)\n",
    "    if(add_rollingMeans):\n",
    "        item_data = pd.concat([item_data, item_data_one_hot], axis = 1)\n",
    "else:\n",
    "    # Set item data to an empty DataFrame with the correct width\n",
    "    item_data = pd.DataFrame(np.zeros((0,30490+additional_features_train_valid.shape[1])))\n",
    "    if(add_rollingMeans):\n",
    "        item_data = pd.concat([item_data, pd.DataFrame(np.zeros((0,30490)))], axis = 1)\n",
    "print(item_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed item data\n",
    "with open(outputPath + \"/item_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(item_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the validation and evaluation parts of the additional features\n",
    "with open(outputPath + \"/additional_features_testing.pkl\", \"wb\") as f:\n",
    "    pickle.dump((additional_features_valid, additional_features_eval), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed (train + validation) data\n",
    "with open(outputPath + \"/preprocessed_train_valid_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_train_valid, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
