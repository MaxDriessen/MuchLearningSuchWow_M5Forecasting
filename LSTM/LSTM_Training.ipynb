{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuchLearningSuchWow - LSTM - Training\n",
    "\n",
    "This notebook contains the code we used to define and train our LSTM network to predict sales for the validation and evaluation datasets. The training code is based loosely on [this kernel](https://www.kaggle.com/bountyhunters/baseline-lstm-with-keras-0-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"input/m5-forecasting-accuracy/\"\n",
    "outputPath = \"output/\"\n",
    "modelPath = \"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 14 # Number of previous days that will be used to predict the next day\n",
    "startDay = 1000 # Number of days at start of data that will be ignored during training\n",
    "\n",
    "# Model settings\n",
    "add_1dConv = True\n",
    "node_setup = (50, 400, 400)\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Training loop settings\n",
    "nr_epochs = 40\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputPath + \"/preprocessed_train_valid_data.pkl\", \"rb\") as f:\n",
    "    df_train_valid = pickle.load(f)\n",
    "with open(outputPath + \"/item_data.pkl\", \"rb\") as f:\n",
    "    item_data = pickle.load(f)\n",
    "print(df_train_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Kaggle validation data for the validation model\n",
    "df_train = df_train_valid[:-28] \n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM():\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1D convolution layer\n",
    "    if(add_1dConv):\n",
    "        model.add(Conv1D(filters=32, kernel_size=7, strides=1, \n",
    "                         padding=\"causal\", activation=\"relu\",\n",
    "                         input_shape=(item_data.shape[0]+timesteps, # == X_train.shape[1]\n",
    "                                      df_train.shape[1]))) # == X_train.shape[2] (df_train.shape[1] == df_train_valid.shape[1])\n",
    "\n",
    "    # LSTM layers\n",
    "    layer_1_units, layer_2_units, layer_3_units = node_setup\n",
    "    \n",
    "    if(add_1dConv):\n",
    "        model.add(LSTM(units = layer_1_units, return_sequences = True))\n",
    "    else:\n",
    "        model.add(LSTM(units = layer_1_units, \n",
    "                       return_sequences = True, \n",
    "                       input_shape=(item_data.shape[0]+timesteps, df_train.shape[1])))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(LSTM(units = layer_2_units, return_sequences = True))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(LSTM(units = layer_3_units))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(units = 30490))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_LSTM()\n",
    "plot_model(model, modelPath + \"/model.png\")\n",
    "print(model.summary())\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (RMSE)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "df_train_scaled = valid_scaler.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Training Data & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid = []\n",
    "y_train_valid = []\n",
    "for i in range(timesteps, 1913 - startDay):\n",
    "    X_train_valid.append(np.append(item_data, df_train_scaled[i-timesteps:i], axis = 0))\n",
    "    y_train_valid.append(df_train_scaled[i][0:30490]) # Only use first 30490 columns (sales) as labels\n",
    "del df_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to np array to be able to feed it to the model\n",
    "X_train_valid = np.array(X_train_valid)\n",
    "y_train_valid = np.array(y_train_valid)\n",
    "print(X_train_valid.shape)\n",
    "print(y_train_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation model\n",
    "valid_model = create_LSTM()\n",
    "\n",
    "# Compile the validation model\n",
    "valid_model.compile(optimizer = Adam(learning_rate=learning_rate), loss = root_mean_squared_error)\n",
    "\n",
    "# Fit the validation model to the validation training set\n",
    "valid_history = valid_model.fit(X_train_valid, y_train_valid, \n",
    "                                epochs = nr_epochs, \n",
    "                                batch_size = batch_size, \n",
    "                                verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_valid\n",
    "del y_train_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data for the evaluation model\n",
    "eval_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "df_train_valid_scaled = eval_scaler.fit_transform(df_train_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Training Data & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data and labels for the evaluation model\n",
    "X_train_eval = []\n",
    "y_train_eval = []\n",
    "for i in range(timesteps, 1941 - startDay):\n",
    "    X_train_eval.append(np.append(item_data, df_train_valid_scaled[i-timesteps:i], axis = 0))\n",
    "    y_train_eval.append(df_train_valid_scaled[i][0:30490]) # Only use first 30490 columns (sales) as labels\n",
    "del df_train_valid_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to np array to be able to feed it to the model\n",
    "X_train_eval = np.array(X_train_eval)\n",
    "y_train_eval = np.array(y_train_eval)\n",
    "print(X_train_eval.shape)\n",
    "print(y_train_eval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation model\n",
    "eval_model = create_LSTM()\n",
    "\n",
    "# Compile the evaluation model\n",
    "eval_model.compile(optimizer = Adam(learning_rate=learning_rate), loss = root_mean_squared_error)\n",
    "\n",
    "# Fit the validation model to the evaluation training set\n",
    "eval_history = eval_model.fit(X_train_eval, y_train_eval, \n",
    "                              epochs = nr_epochs, \n",
    "                              batch_size = batch_size, \n",
    "                              verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_eval\n",
    "del y_train_eval\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_model.save(modelPath + \"/lstm_model_valid\")\n",
    "eval_model.save(modelPath + \"/lstm_model_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputPath + \"/scalers.pkl\", \"wb\") as f:\n",
    "    pickle.dump((valid_scaler, eval_scaler), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, nr_epochs, title):\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1, nr_epochs+1), history.history['loss'], label='train_loss')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(valid_history, nr_epochs, 'Validation Model Training Loss')\n",
    "plot_loss(eval_history, nr_epochs, 'Evaluation Model Training Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
